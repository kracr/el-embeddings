{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for modifying raw RDF ontology file to use for training Owl2Vec*\n",
    "* Create 3 files, each containing relations of only one kind\n",
    "* Remove test axioms to create train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by relation types (in rdfxml file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_path.append(\"C:/Users/Abhishek/Desktop/Semester-8/IP/data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DATASET in [\"GALEN\"]:\n",
    "  for split in [\"1_1\", \"1_n\", \"n_n\"]:\n",
    "    if DATASET == \"SNOMED\":\n",
    "      onto = get_ontology(\"snomed.owl\").load(\n",
    "        reload=True, only_local=True\n",
    "        )\n",
    "      splitter = \"#\"\n",
    "    if DATASET == \"GALEN\":\n",
    "      onto = get_ontology(\"galen.owl\").load(reload=True, only_local=True)\n",
    "      splitter = \"#\"\n",
    "    if DATASET == \"GO\":\n",
    "      onto = get_ontology(\"go.owl\").load(reload=True, only_local=True)\n",
    "      splitter = \"/\"\n",
    "    print(DATASET, split)\n",
    "    print(len(list(onto.object_properties())))\n",
    "    relations = []\n",
    "    with open(DATASET+\"/\"+split+\"/\"+DATASET+\"_\"+split+\"_relations.txt\") as f:\n",
    "      for line in f:\n",
    "        line = line.strip(\"\\n\").strip(\">\").strip(\"<\")\n",
    "        # print(line)\n",
    "        relations.append(\n",
    "          line.split(splitter)[-1]\n",
    "        )\n",
    "    print(len(relations))\n",
    "    # relations = []\n",
    "    for rel in onto.object_properties():\n",
    "      name = str(rel).split(\".\")[-1]\n",
    "      # print(name)\n",
    "      # break\n",
    "      if name in all_relations and name not in relations:\n",
    "        try:\n",
    "          print(\"destroying\", rel)\n",
    "          destroy_entity(rel)\n",
    "        except:\n",
    "          print(\"couldn't destroy \", name)\n",
    "          # continue\n",
    "      # break\n",
    "        # print(\"deleting\", name)\n",
    "    filename = \"raw/\"+DATASET+\"_\"+split+\".owl\"\n",
    "    print(onto, len(list(onto.properties())), list(onto.properties()))\n",
    "    onto.save(file = filename, format = \"rdfxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove test axioms from RDF file to create training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = {\n",
    "  \"GALEN\": (\n",
    "    \"raw/galen_{}.owl\", # input file eg. galen_1_1.owl\n",
    "    \"GALEN/{}/{}.txt\", # test file eg. test.txt\n",
    "    \"raw/galen_{}_without_test.owl\", # output file eg. galen_1_1_without_test.owl\n",
    "    \"galen.\" # ontology prefix\n",
    "  ),\n",
    "  \"GO\": (\n",
    "    \"raw/GO_{}.owl\",\n",
    "    \"GO/{}/{}.txt\",\n",
    "    \"raw/go_{}_without_test.owl\",\n",
    "    \"obo.\"\n",
    "  ),\n",
    "  \"SNOMED\": (\n",
    "    \"raw/SNOMED_{}.owl\",\n",
    "    \"SNOMED/{}/{}.txt\",\n",
    "    \"raw/snomed_{}_without_test.owl\",\n",
    "    \"train.\"\n",
    "  ),\n",
    "  # \"owl2bench\": (\n",
    "  #   \"OWL2EL_5/{}/train.owl\",\n",
    "  #   \"OWL2EL_5/{}/test.txt\",\n",
    "  #   \"OWL2EL_5/OWL2EL_5_{}_without_test.owl\"\n",
    "  # )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTest(onto, test, prefix=\"galen.\"):\n",
    "  for a,b in tqdm(test):\n",
    "    classes = onto.search(iri = \"*\"+a)\n",
    "    if classes == []:\n",
    "      continue\n",
    "    for cls in classes:\n",
    "      if str(cls) != prefix+a:\n",
    "        continue\n",
    "      cls.is_a = [subcls for subcls in cls.is_a if str(subcls) != prefix+b]\n",
    "  return onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/26729 [00:00<36:45, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26729/26729 [34:34<00:00, 12.89it/s] \n",
      "  0%|          | 2/26729 [00:00<32:45, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26729/26729 [33:15<00:00, 13.40it/s] \n",
      "  0%|          | 2/36464 [00:00<43:09, 14.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36464/36464 [15:02:26<00:00,  1.48s/it]       \n"
     ]
    }
   ],
   "source": [
    "for DATASET in [\"GO\"]:\n",
    "  for split in [\"1_1\", \"1_n\", \"n_n\"]:\n",
    "    onto = get_ontology(file[DATASET][0].format(split)).load(reload=True, only_local=True)\n",
    "    # Get test pts\n",
    "    test_pts = []\n",
    "    sep = \"/\"\n",
    "    with open(file[DATASET][1].format(split, \"test\")) as f:\n",
    "      for line in f:\n",
    "        line = line.strip(\"\\n\")\n",
    "        a, b = line.split()\n",
    "        c = a.strip().strip(\">\").split(sep)[-1]\n",
    "        d = b.strip().strip(\">\").split(sep)[-1]\n",
    "        test_pts.append((c,d))\n",
    "    with open(file[DATASET][1].format(split, \"valid\")) as f:\n",
    "      for line in f:\n",
    "        line = line.strip(\"\\n\")\n",
    "        a, b = line.split()\n",
    "        c = a.strip().strip(\">\").split(sep)[-1]\n",
    "        d = b.strip().strip(\">\").split(sep)[-1]\n",
    "        test_pts.append((c,d))\n",
    "    print(len(test_pts))\n",
    "    onto = removeTest(onto, test_pts, file[DATASET][3])\n",
    "    onto.save(file = file[DATASET][2].format(split), format = \"rdfxml\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b1309862175574e3ed2ff1dbc2774d81834181e2009982081f5e92b54f53616"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
